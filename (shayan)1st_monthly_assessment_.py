# -*- coding: utf-8 -*-
"""(Shayan)1st Monthly Assessment .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xOH-4UtgokDPe9oBdnCUYH7mDmWPVFZe
"""

# Section B â€“ Short Questions (20 Marks)

# Q1) Explain the difference between a Python list and a NumPy array.?

# Ans) Python List:
# 1) Python list Can hold mixed data types (e.g., int, float, string in one list).
# 2) Limited support for mathematical operations.
# 3) Basic slicing and indexing.
# 4) Slower for large numerical computations.

#   Numpy array:
# 1) Numpy array Holds elements of the same data type (more efficient).
# 2) Lower memory usage, especially with large data.
# 3) Scientific computing, data analysis, machine learning, etc.
# 4) Faster and more efficient for numerical operations due to optimized C backend.

# PART "C" QUESTION NO 1
import pandas as pd
import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt
shayan = sns.load_dataset('titanic')
print(shayan.head())
print("Shape:", shayan.shape)
print(shayan.info())
print(shayan.describe(include='all'))

shayan1= shayan.select_dtypes(include=['float64', 'int64'])

# Plot pairplot
sns.pairplot(shayan1)
plt.suptitle("Pairplot of Titanic Numerical Features", y=1.02)
plt.show()

titanic_clean = shayan.dropna(subset=['age', 'fare'])
fig = px.scatter(
    titanic_clean,
    x='age',
    y='fare',
    color='sex',
    hover_data=['class', 'embarked', 'survived'],
    title='Titanic: Age vs Fare (Interactive)'
)
fig.show()

import pandas as pd
import seaborn as sns
import plotly.express as px
titanic_df = sns.load_dataset('titanic')
titanic_df = titanic_df.dropna(subset=['age', 'fare'])
fig = px.scatter(
    titanic_df,
    x='age',
    y='fare',
    color='sex',  # You can also use 'survived' if you want
    hover_data=['class', 'embarked', 'survived'],
    title='Titanic: Age vs Fare (Interactive)'
)

fig.show()

# QUESTION NO 2

# Imports
import pandas as pd
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

df = sns.load_dataset('titanic')
print("Missing values per column:\n", df.isnull().sum())
df = df.drop(['deck'], axis=1)
df['age'] = df['age'].fillna(df['age'].median())
df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])
print("\nMissing values after handling:\n", df.isnull().sum())

if df['survived'].dtype == 'object':
    le = LabelEncoder()
    df['survived'] = le.fit_transform(df['survived'])

# Select features and target
features = ['pclass', 'sex', 'age', 'fare', 'embarked']
target = 'survived'
df_encoded = pd.get_dummies(df[features], drop_first=True)
X = df_encoded
y = df[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

# Use NumPy on original (filled) age column
age_values = df['age'].values
print("Age - Mean:", np.mean(age_values))
print("Age - Median:", np.median(age_values))
print("Age - Standard Deviation:", np.std(age_values))

# QUESTION NO 3

from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Train SVM model
svm_model = SVC()
svm_model.fit(X_train, y_train)
svm_preds = svm_model.predict(X_test)
svm_accuracy = accuracy_score(y_test, svm_preds)
print("SVM Accuracy:", round(svm_accuracy, 4))

# Train Decision Tree
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)
dt_preds = dt_model.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_preds)
dt_depth = dt_model.get_depth()
dt_leaves = dt_model.get_n_leaves()
print("Decision Tree Accuracy:", round(dt_accuracy, 4))
print("Decision Tree Depth:", dt_depth)
print("Decision Tree Leaves:", dt_leaves)

# Train Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_preds)
feature_importances = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)
print("Random Forest Accuracy:", round(rf_accuracy, 4))
print("Top 5 Feature Importances:")
print(feature_importances.head())

# Create comparison table
results = pd.DataFrame({
    'Model': ['Support Vector Machine', 'Decision Tree', 'Random Forest'],
    'Accuracy': [round(svm_accuracy, 4), round(dt_accuracy, 4), round(rf_accuracy, 4)],
    'Remarks': [
        'Basic SVC with default params',
        f'Depth: {dt_depth}, Leaves: {dt_leaves}',
        'n_estimators=100, feature importance printed'
    ]
})
print("\nModel Comparison:\n")
print(results)

# Question No 4

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import seaborn as sns
def plot_confusion(model_name, y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap='Blues')
    plt.title(f'{model_name} - Confusion Matrix')
    plt.show()
plot_confusion("SVM", y_test, svm_preds)
plot_confusion("Decision Tree", y_test, dt_preds)
plot_confusion("Random Forest", y_test, rf_preds)

# Plot feature importance using seaborn
plt.figure(figsize=(8, 5))
sns.barplot(x=feature_importances.values, y=feature_importances.index, palette="viridis")
plt.title("Random Forest - Feature Importance")
plt.xlabel("Importance")
plt.ylabel("Features")
plt.tight_layout()
plt.show()

# Reuse the results DataFrame from earlier
plt.figure(figsize=(7, 4))
sns.barplot(data=results, x='Model', y='Accuracy', palette='Set2')
plt.title('Model Accuracy Comparison')
plt.ylim(0, 1)
plt.ylabel('Accuracy')
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

import plotly.express as px
fig = px.bar(
    results,
    x='Model',
    y='Accuracy',
    color='Model',
    text='Accuracy',
    title='Interactive Comparison of Model Accuracies (Plotly)',
    labels={'Accuracy': 'Accuracy Score'}
)
fig.update_traces(texttemplate='%{text:.2%}', textposition='outside')
fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide', yaxis=dict(range=[0, 1]))
fig.show()

# Model Performance Analysis

# Among the three models, **Random Forest Classifier** performed the best with the highest accuracy.
# This is likely because Random Forest is an ensemble method that reduces overfitting and captures more complex patterns than a single Decision Tree.
# The Decision Tree performed reasonably well but tends to overfit on small datasets.
# The SVM showed good performance but may be sensitive to parameter settings and doesn't naturally handle categorical variables unless encoded well.
# Overall, Random Forest's ability to average out errors across multiple trees made it the most robust model for this Titanic dataset.